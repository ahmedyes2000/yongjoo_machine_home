\section{The Bipartite Graph Model}
\subsection{Graph Construction}

We first formulate a classification task with the following parameters:

\begin{enumerate}[a.]

    \item A set of $n$ examples, among which $l$ are labeled, $u$ are unlabeled;
        $n = l + u$, usually $n >> l$.
        The $l$ labeled examples and their labels form a labeled set 
        $L:= \{(x_1, y_1), (x_2, y_2), ..., (x_l, y_l)\}$;

    \item The $u$ unlabeled examples form an unlabeled set 
        $U:= \{x_{l+1}, x_{l+2}, ... x_{l+u}\}$; 

    \item Each example's feature is a subset of the norminal feature set 
        $F = \{f_1, f_2, ..., f_m\}$. 
        Write $x_i = (x_i^1, x_i^2, ..., x_i^m)$, and $x_i^j =
        1$ if example $x_i$ has nominal feature $f_j$, otherwise $x_i^j = 0$. 

\end{enumerate}
The goal is to predict the labels of any (all) unlabeled examples.

To construct the bipartite graph $G = (X, F, E)$ ($X, F$ are node sets and $E$
is edge set), we create examples nodes for each
example $X=\{x_1, x_2, ..., x_n\}$, and feature nodes for each nominal feature
$F=\{f_1, f_2, ..., f_m\}$.
We connect an example node and a feature node using an undirected
edge if and only if the feature is present in the example, $(x_i, f_i) \in E
\mbox{ iff } x_i^j = 1$.  


\subsection{Random Walk}

// Describe the random walk hitting time method.

// add algorithm


\subsection{Label Propagation}


An alternative impelmentation that solves the same problems as performing random
walks is label propagation.


// add algorithm of tumbl

// explain or prove the equivalence of the two algorithms theoretically

// cite Zhu's 2002 paper

\subsection{Active Learning}
