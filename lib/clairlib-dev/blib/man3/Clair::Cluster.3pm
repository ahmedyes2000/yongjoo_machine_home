.\" Automatically generated by Pod::Man 2.25 (Pod::Simple 3.04)
.\"
.\" Standard preamble:
.\" ========================================================================
.de Sp \" Vertical space (when we can't use .PP)
.if t .sp .5v
.if n .sp
..
.de Vb \" Begin verbatim text
.ft CW
.nf
.ne \\$1
..
.de Ve \" End verbatim text
.ft R
.fi
..
.\" Set up some character translations and predefined strings.  \*(-- will
.\" give an unbreakable dash, \*(PI will give pi, \*(L" will give a left
.\" double quote, and \*(R" will give a right double quote.  \*(C+ will
.\" give a nicer C++.  Capital omega is used to do unbreakable dashes and
.\" therefore won't be available.  \*(C` and \*(C' expand to `' in nroff,
.\" nothing in troff, for use with C<>.
.tr \(*W-
.ds C+ C\v'-.1v'\h'-1p'\s-2+\h'-1p'+\s0\v'.1v'\h'-1p'
.ie n \{\
.    ds -- \(*W-
.    ds PI pi
.    if (\n(.H=4u)&(1m=24u) .ds -- \(*W\h'-12u'\(*W\h'-12u'-\" diablo 10 pitch
.    if (\n(.H=4u)&(1m=20u) .ds -- \(*W\h'-12u'\(*W\h'-8u'-\"  diablo 12 pitch
.    ds L" ""
.    ds R" ""
.    ds C` ""
.    ds C' ""
'br\}
.el\{\
.    ds -- \|\(em\|
.    ds PI \(*p
.    ds L" ``
.    ds R" ''
'br\}
.\"
.\" Escape single quotes in literal strings from groff's Unicode transform.
.ie \n(.g .ds Aq \(aq
.el       .ds Aq '
.\"
.\" If the F register is turned on, we'll generate index entries on stderr for
.\" titles (.TH), headers (.SH), subsections (.SS), items (.Ip), and index
.\" entries marked with X<> in POD.  Of course, you'll have to process the
.\" output yourself in some meaningful fashion.
.ie \nF \{\
.    de IX
.    tm Index:\\$1\t\\n%\t"\\$2"
..
.    nr % 0
.    rr F
.\}
.el \{\
.    de IX
..
.\}
.\"
.\" Accent mark definitions (@(#)ms.acc 1.5 88/02/08 SMI; from UCB 4.2).
.\" Fear.  Run.  Save yourself.  No user-serviceable parts.
.    \" fudge factors for nroff and troff
.if n \{\
.    ds #H 0
.    ds #V .8m
.    ds #F .3m
.    ds #[ \f1
.    ds #] \fP
.\}
.if t \{\
.    ds #H ((1u-(\\\\n(.fu%2u))*.13m)
.    ds #V .6m
.    ds #F 0
.    ds #[ \&
.    ds #] \&
.\}
.    \" simple accents for nroff and troff
.if n \{\
.    ds ' \&
.    ds ` \&
.    ds ^ \&
.    ds , \&
.    ds ~ ~
.    ds /
.\}
.if t \{\
.    ds ' \\k:\h'-(\\n(.wu*8/10-\*(#H)'\'\h"|\\n:u"
.    ds ` \\k:\h'-(\\n(.wu*8/10-\*(#H)'\`\h'|\\n:u'
.    ds ^ \\k:\h'-(\\n(.wu*10/11-\*(#H)'^\h'|\\n:u'
.    ds , \\k:\h'-(\\n(.wu*8/10)',\h'|\\n:u'
.    ds ~ \\k:\h'-(\\n(.wu-\*(#H-.1m)'~\h'|\\n:u'
.    ds / \\k:\h'-(\\n(.wu*8/10-\*(#H)'\z\(sl\h'|\\n:u'
.\}
.    \" troff and (daisy-wheel) nroff accents
.ds : \\k:\h'-(\\n(.wu*8/10-\*(#H+.1m+\*(#F)'\v'-\*(#V'\z.\h'.2m+\*(#F'.\h'|\\n:u'\v'\*(#V'
.ds 8 \h'\*(#H'\(*b\h'-\*(#H'
.ds o \\k:\h'-(\\n(.wu+\w'\(de'u-\*(#H)/2u'\v'-.3n'\*(#[\z\(de\v'.3n'\h'|\\n:u'\*(#]
.ds d- \h'\*(#H'\(pd\h'-\w'~'u'\v'-.25m'\f2\(hy\fP\v'.25m'\h'-\*(#H'
.ds D- D\\k:\h'-\w'D'u'\v'-.11m'\z\(hy\v'.11m'\h'|\\n:u'
.ds th \*(#[\v'.3m'\s+1I\s-1\v'-.3m'\h'-(\w'I'u*2/3)'\s-1o\s+1\*(#]
.ds Th \*(#[\s+2I\s-2\h'-\w'I'u*3/5'\v'-.3m'o\v'.3m'\*(#]
.ds ae a\h'-(\w'a'u*4/10)'e
.ds Ae A\h'-(\w'A'u*4/10)'E
.    \" corrections for vroff
.if v .ds ~ \\k:\h'-(\\n(.wu*9/10-\*(#H)'\s-2\u~\d\s+2\h'|\\n:u'
.if v .ds ^ \\k:\h'-(\\n(.wu*10/11-\*(#H)'\v'-.4m'^\v'.4m'\h'|\\n:u'
.    \" for low resolution devices (crt and lpr)
.if \n(.H>23 .if \n(.V>19 \
\{\
.    ds : e
.    ds 8 ss
.    ds o a
.    ds d- d\h'-1'\(ga
.    ds D- D\h'-1'\(hy
.    ds th \o'bp'
.    ds Th \o'LP'
.    ds ae ae
.    ds Ae AE
.\}
.rm #[ #] #H #V #F C
.\" ========================================================================
.\"
.IX Title "Clair::Cluster 3pm"
.TH Clair::Cluster 3pm "2012-07-09" "perl v5.14.2" "User Contributed Perl Documentation"
.\" For nroff, turn off justification.  Always turn off hyphenation; it makes
.\" way too many mistakes in technical documents.
.if n .ad l
.nh
.SH "NAME"
Clair::Cluster \- The great new Clair::Cluster!
.SH "VERSION"
.IX Header "VERSION"
Version 0.01
.SH "SYNOPSIS"
.IX Header "SYNOPSIS"
Quick summary of what the module does.
.PP
Perhaps a little code snippet.
.PP
use Clair::Cluster;
.PP
my \f(CW$foo\fR = Clair::Cluster\->new( id => \*(L"myCluster\*(R" );
\&...
.SH "EXPORT"
.IX Header "EXPORT"
A list of functions that can be exported.  You can delete this section
if you don't export anything, such as for a purely object-oriented module.
.SH "FUNCTIONS"
.IX Header "FUNCTIONS"
.SS "new"
.IX Subsection "new"
\&\f(CW%documents\fR = ($id1, \f(CW$doc1\fR, \f(CW$id2\fR, \f(CW$doc2\fR);
\&\f(CW$cluster\fR = new Clair::Cluster(documents => \e%documents, id => \*(L"myCluster\*(R");
.PP
Creates a cluster with the specified documents.  If no documents are specified,
        creates an empty cluster. An optional id parameter can be passed to
        identify this cluster with the get_id method.
.PP
.Vb 1
\&        =cut
\&
\&        =head2 get_id
\&
\&        Returns the id of this cluster, specified in the constructor. Returns undef
\&        if no id is specified.
\&
\&        =head2 insert
.Ve
.PP
insert($id, \f(CW$document\fR)
.PP
.Vb 1
\&        Insert the document into the cluster with the associated id
\&
\&        =cut
\&
\&
\&        =head2 get
.Ve
.PP
get($id)
.PP
.Vb 1
\&        Returns the document from the cluster with that id
\&
\&        =cut
\&
\&
\&        =head2 count_elements
\&
\&        count_elements
\&
\&        Returns the number of documents in the cluster
\&
\&        =cut
\&
\&
\&        =head2 documents_by_class
\&
\&        %docs_by_class = $c\->documents_by_class();
\&
\&        Returns a hash whose keys correspond to classes,
\&        each value of which is a reference to a hash with
\&        keys corresponding to the document ids belonging
\&        to that class. If a document\*(Aqs class is undefined,
\&        its id is not contained anywhere in this hash.
\&
\&        =cut
\&
\&
\&        =head2 get_class
.Ve
.PP
\&\f(CW$label\fR = \f(CW$c\fR\->get_class($did)
.PP
.Vb 2
\&        Returns the class of the document having the
\&        specified id.
\&
\&        =cut
\&
\&
\&        =head2 set_class
.Ve
.PP
\&\f(CW$c\fR\->set_class($did, \f(CW$label\fR)
.PP
.Vb 2
\&        Sets the class of the document having id $did
\&        to $label.
\&
\&        =cut
\&
\&
\&        =head2 tf
\&
\&        %tf = $c\->tf(type => "stem")
\&
\&        Splits each document in the cluster into terms of the given type,
\&        then returns a hash containing the term frequencies for the entire
\&        cluster.
\&
\&        =cut
\&
\&        =head2 docterm_matrix
\&
\&        (\e@matrix, \e@docids, \e@uniqterms) = $c\->docterm_matrix(type => "stem")
\&
\&        Returns 1) a reference to a document\-term matrix over the set of
\&        all the documents and terms in the cluster. Each element of
\&        @matrix is a pointer to a bag\-of\-words vector representation
\&        for a document. Also returns 2) a reference to an array containing
\&        all document IDs in the cluster, sorted alphabetically, and 3) a
\&        reference to an array containing all words occurring in the cluster,
\&        also sorted alphabetically.
\&
\&        =cut
\&
\&
\&        =head2 classes
.Ve
.PP
\&\f(CW%classes\fR = \f(CW$c\fR\->\fIclasses()\fR
.PP
.Vb 3
\&        Returns a hash whose keys correspond to classes,
\&        each value of which equals the number of documents
\&        in the cluster belonging to that class.
\&
\&        =cut
\&
\&
\&        =head2 compute_cosine_matrix
\&
\&        %cos_hash = compute_cosine_matrix(text_type => \*(Aqstem\*(Aq)
\&
\&        Computes the cosine matrix of the documents in the cluster.  Uses the stemmed version of the
\&        documents unless text_type is specified (can be \*(Aqhtml\*(Aq, \*(Aqtext\*(Aq, or \*(Aqstem\*(Aq).  Result is a two\-
\&        dimensional hash\-\-get the cosine using $cos_hash{doc1_key}{doc2_key}.
\&
\&        The result is stored with the class in addition to being returned.
\&
\&        =cut
\&
\&        =head2 create_lexical_network
.Ve
.PP
\&\fIcreate_lexical_network()\fR
.PP
.Vb 1
\&        Convert a cluster of documents to a lexical network
\&
\&        Each word in the document is a node. A edge exists between two words if they
\&        occur in the same sentence.  The weight of an edge corresponds to the number
\&        of times those words occur together.
\&
\&        =cut
\&
\&        =head2 get_largest_cosine
\&
\&        %largest_cosine_hash = get_largest_cosine(cosine_matrix => \e%cos_matrix);
\&
\&        Returns a hash containing the value of the largest cosine and the keys corresponding
\&        to that value.  Uses the cosine matrix calculated by compute_cosine_matrix unless one is
\&        provided.
\&
\&        The value of the largest cosine is stored with the key \*(Aqvalue\*(Aq, while the two keys that
\&        correspond to the largest cosine are stored in the hash with keys \*(Aqkey1\*(Aq and \*(Aqkey2\*(Aq.
\&
\&        =cut
\&
\&
\&        =head2 compute_binary_cosine
\&
\&        compute_binary_cosine
\&
\&        Computes the binary cosine using the cosine matrix calculated by compute_cosine_matrix.
\&        Returns the cosine hash, similairly to compute_cosine_matrix.  Note that the binary
\&        cosine is NOT stored with the class.
\&
\&        =cut
\&
\&
\&        =head2 create_network
\&
\&        $n = create_network(cosine_matrix => \e%cos_matrix, include_zeros => 1);
\&
\&        Creates a network using the provided cosine matrix.  If no cosine matrix is specified,
\&        the one computed by compute_cosine_matrix is used.  Unless include_zeros is specified
\&        and is equal to 1, all documents that have a cosine of zero between them are not
\&        connected on the graph.
\&
\&        =cut
\&
\&
\&        =head2 write_cos
\&
\&        write_cos($file, cosine_matrix => \e%cos_matrix);
\&
\&        Writes the cosine matrix to a file.  If no cosine matrix is specified, the one compute
\&        by compute_cosine_matrix is used.
\&
\&        =cut
\&
\&
\&        =head2 save_documents_to_file
.Ve
.PP
save_documentss_to_file($filename, \f(CW$type\fR)
.PP
.Vb 2
\&        Save the documents to a single file, one document per line.
\&        Only really makes sense for sentence\-based documents.
\&
\&        =cut
\&
\&
\&        =head2 build_idf
\&
\&        build_idf($dbm_file, type => \*(Aqtext\*(Aq)
\&
\&        Computes idf values from the documents in the cluster.  Returns a hash of each word
\&        to the idf value.  The type parameter is optional, the default is \*(Aqtext\*(Aq, but it can
\&        also be set to \*(Aqstem\*(Aq or \*(Aqhtml\*(Aq.
\&
\&        =cut
\&
\&
\&        =head2 create_hyperlink_network_from_array
\&
\&        create_hyperlink_network_from_array(\e@array, property => \*(Aqpagerank_transition\*(Aq)
\&
\&        Creates a network based with a link for each hyperlink in the array.  Each hyperlink
\&        should be represented as an array with the source, then the destination.
\&
\&        The pagerank_transition property will be set appropriately so that pagerank can be
\&        run later, but another property can be set instead by defining the optional
\&        property parameter.
\&
\&        =cut
\&
\&
\&        =head2 create_hyperlink_network_from_file
\&
\&        create_hyperlink_network_from_file($filename, property => \*(Aqpagerank_transition\*(Aq)
\&
\&        Creates a network based with a link for each hyperlink in the file.  Each hyperlink
\&        should be represented as a line in the file with the source, a space, and then the
\&        destination.
\&
\&        The pagerank_transition property will be set appropriately so that pagerank can be
\&        run later, but another property can be set instead by defining the optional
\&        property parameter.
\&
\&        =cut
\&
\&
\&        =head2 create_sentence_based_cluster
\&
\&        create_sentence_based_cluster
\&
\&        Creates a new cluster containing the sentences of each document from the original cluster.
\&        Each sentence becomes a new Clair::Document with the document it came from set as the parent
\&        document.  Its id is the parent\*(Aqs id with the sentence number appended to it (for example,
\&                        if it\*(Aqs the first sentence in a document with id \*(Aqblue\*(Aq, it\*(Aqs new id will be \*(Aqblue1\*(Aq).
\&
\&        =cut
\&
\&
\&        =head2 create_sentence_based_network
.Ve
.PP
create_sentence_based_network(threshold => 0.2, include_zeros => 0)
.PP
.Vb 2
\&        Creates a new network containing the sentences of each document from the cluster and links
\&        for each node with an appropriate lexical similarity.
\&
\&        Each sentence becomes a new Clair::Document with the document it came from set as the parent
\&        document.  Its id is the parent\*(Aqs id with the sentence number appended to it (for example,
\&                        if it\*(Aqs the first sentence in a document with id \*(Aqblue\*(Aq, it\*(Aqs new id will be \*(Aqblue1\*(Aq).
\&
\&        The lexical similarity is computed for the new cluster.  If an optional threshold is specified
\&        that is not zero, then similarities that are less than the threshold are set to zero.
\&
\&        A link is only made if the lexical similarity between two sentences is greater than zero OR the
\&        optional parameter include_zeros has been set to 1.
\&
\&
\&        =cut
\&
\&
\&        =head2 load_documents
\&
\&        load_documents("docs/*.txt", type => \*(Aqtext\*(Aq, filename_id => 1)
\&
\&        Loads all documents matching the expression given as the first parameter into the cluster.
\&
\&        If the optional type is provided, then each document is given that type, or text as the
\&        default.  The id of the document will be the filename, unless optional parameter
\&        filename_id is specified as 0 or optional parameter filename_count is specified as 1, in which
\&        case each document will be specified a unique number (the first document given 1, the second 2,
\&                        and so on).
\&
\&        =cut
\&
\&
\&        =head2 load_file_list_array
\&
\&        load_file_list_array($filename, type => \*(Aqtext\*(Aq, filename_id => 1)
\&
\&        Loads all the documents in the array given as the first parameter
\&        and adds them to the cluster.
\&
\&        If the optional type is provided, then each document is given that type, or text as the
\&        default.  The id of the document will be the filename, unless optional parameter
\&        filename_id is specified as 0 or optional parameter filename_count is specified as 1, in which
\&        case each document will be specified a unique number (the first document given 1, the second 2,
\&                        and so on).
\&
\&        =cut
\&
\&
\&        =head2 load_file_list_from_file
\&
\&        load_file_list_from_file($filename, type => \*(Aqtext\*(Aq, filename_id => 1)
\&
\&        Loads the documents listed in the file whose name is given as the
\&        first parameter and adds them to the cluster.  Each file should be listed alone on a line.
\&
\&        If the optional type is provided, then each document is given that type, or text as the
\&        default.  The id of the document will be the filename, unless optional parameter
\&        filename_id is specified as 0 or optional parameter filename_count is specified as 1, in which
\&        case each document will be specified a unique number (the first document given 1, the second 2,
\&                        and so on).
\&
\&        =cut
\&
\&
\&        =head2 load_sentences_from_file
\&
\&        load_sentences_from_file($filename, type => \*(Aqtext\*(Aq, id_prefix => \*(Aq\*(Aq)
\&
\&        Loads each sentence from a file as a separate document and adds it to the cluster.
\&
\&        If the optional type parameter is specified, the new documents will be created as that
\&        type (text is the default).  If an id_prefix is specified, that string will be prepended
\&        to each sentence\*(Aqs number to form the id.
\&
\&        =cut
\&
\&        =head2 load_corpus
\&
\&        Load a corpus directory into a cluster
\&        Pass in a Clair::Corpus object
\&
\&        =cut
\&
\&        =head2 save_documents_to_directory
\&
\&        save_documents_to_directory($directory, \*(Aqtext\*(Aq, name_count => 1)
\&
\&        Saves each document from the cluster to the specified directory.  The second parameter
\&        specifies whether the html, text, or stem version of the document is saved.  If the
\&        optional parameter name_count is set to 0 or the optional parameter name_id is set to 1,
\&        the document\*(Aqs id is used as the filename.  Otherwise (and by default), the first document
\&        saved is saved with filename \*(Aq1\*(Aq, the second with filename \*(Aq2\*(Aq, and so on.
\&
\&        =cut
\&
\&
\&        =head2 stem_all_documents
\&
\&        stem_all_documents
\&
\&        Goes through each document in the cluster and calls stem on it.
\&
\&        =cut
\&
\&
\&        =head2 strip_all_documents
\&
\&        strip_all_documents
\&
\&        Goes through each document in the cluster and calls strip_html on it.
\&
\&        =cut
\&
\&
\&        =head2 documents
\&
\&        documents
\&
\&        Returns the hash of documents in the cluster.
\&
\&        =cut
\&
\&
\&        =head2 compute_lexrank
\&
\&        Computes lexrank on this cluster. Any parameters will be passed to the
\&        Clair::Network method compute_lexrank.
\&
\&        =cut
\&
\&
\&        =head2 get_unique_words
\&
\&        $c\->get_unique_words(type => \*(Aqstem\*(Aq)
\&
\&        Returns a list of unique words out of all the documents in the cluster.
\&        Defaults to extracting these words from stemmed versions of the documents,
\&        but can be set to text or html by passing an optional type argument:
\&        get_unique_words(type => \*(Aqstem\*(Aq)
\&
\&        =cut
\&
\&
\&        =head2 compute_genprob_matrix
\&
\&        my %matrix = $cluster\->compute_genprob_matrix(
\&                        genprob => $path_to_genprob
\&                        );
\&
\&        Computes the generation probability matrix for this cluster. Returns a
\&        hashmap of hashrefs in the form $hash{$id1}\->{$id2} mapping two document
\&        ids to the generation probability of document $id2 given document $id1.
\&        To use this method with LexRank, use the create_genprob_network method.
\&        Takes a parameter "genprob" that maps to the binary executable tf2gen.
\&        This value defaults to the $GENPROB variable set in Clair::Config.
\&
\&
\&        =head2 create_genprob_network
\&
\&        my %genprob = $cluster\->compute_genprob_matrix();
\&        my $network = $cluster\->create_genprob_network(
\&                        genprob_matrix => \e%genprob,
\&                        include_zeros => 1
\&                        );
\&
\&        Creates a Clair::Network object from the given genprob matrix. See the
\&        description for create_network for more information.
\&
\&        =cut
.Ve
.ie n .SS "compute_sentence_features( %features )"
.el .SS "compute_sentence_features( \f(CW%features\fP )"
.IX Subsection "compute_sentence_features( %features )"
.Vb 3
\&        Computes a set of features on all sentences. %features should be a hash
\&        mapping names to feature subroutine references. See compute_sentence_feature
\&        for more information.
\&
\&        =cut
.Ve
.ie n .SS "compute_sentence_feature( name => $name, feature => $subref, normalize => 1 )"
.el .SS "compute_sentence_feature( name => \f(CW$name\fP, feature => \f(CW$subref\fP, normalize => 1 )"
.IX Subsection "compute_sentence_feature( name => $name, feature => $subref, normalize => 1 )"
.Vb 3
\&        Computes the given feature for each sentence in the cluster. The feature
\&        parameter should be a reference to a subroutine. The subroutine will be
\&        called with the following parameters defined:
\&
\&        =over 4
\&
\&        =item * cluster \- a reference to the cluster object
\&
\&        =item * document \- a reference to the document object
\&
\&        =item * sentence \- the sentence text
\&
\&        =item * sentence_index \- the index of the sentence
\&
\&        =item * state \- A hash reference that is kept in memory between calls to the subroutine. This lets $subref save precomputed values or keep track of inter\-sentence relationships.
\&
\&        =back
\&
\&        The parameter cluster is not passed when the same method is called on
\&        L<Clair::Document>. Thus calling compute_sentence_feature from Clair::Cluster
\&        gives an extra cluster context passed to the feature subroutine.
\&
\&        A feature subroutine should return a value. Any exceptions thrown by the
\&        feature subroutine will be caught and a warning will be shown. If a feature
\&        subroutine returns an undefined value, the feature will not be set and a
\&        warning will be shown. This method returns undef if either name or feature
\&        are not defined.
\&
\&        The normalize parameter, if set to a true value, will scale the values of this
\&        feature so that the minimum value is 0 and the maximum value is 1. Nothing
\&        will happen if any of the feature values are non\-numeric.
\&
\&        =cut
.Ve
.SS "normalize_sentence_feature($name)"
.IX Subsection "normalize_sentence_feature($name)"
.Vb 3
\&        Scales the values of the given feature so that the minimum value is 0 and
\&        the maximum value is 1. Nothing will happen if any of the feature values are
\&        non\-numeric.
.Ve
.SS "normalize_sentence_features(@names)"
.IX Subsection "normalize_sentence_features(@names)"
.Vb 2
\&        Scales the values of the given features such that for each feature the
\&        minimum value is 0 and the maximum value is 1.
\&
\&        =cut
.Ve
.ie n .SS "get_sentence_features($did, $i)"
.el .SS "get_sentence_features($did, \f(CW$i\fP)"
.IX Subsection "get_sentence_features($did, $i)"
.Vb 2
\&        Returns a hash mapping the features to values of sentence $i in document
\&        $did.
\&
\&        =cut
.Ve
.ie n .SS "get_sentence_feature($did, $i, $name)"
.el .SS "get_sentence_feature($did, \f(CW$i\fP, \f(CW$name\fP)"
.IX Subsection "get_sentence_feature($did, $i, $name)"
.Vb 4
\&        Returns the value of the given feature $name for the given sentence $i in
\&        the document $did (where $i is the index of the sentence starting at 0).
\&        Returns undef if $did isn\*(Aqt a valid document id, $i isn\*(Aqt in the range of
\&        sentences, or $name isn\*(Aqt a valid feature.
\&
\&        =cut
\&
\&        =head2 remove_sentence_features
\&
\&        Removes all of the features from all of the sentences.
\&
\&        =cut
.Ve
.ie n .SS "set_sentence_feature($did, $i, %features)"
.el .SS "set_sentence_feature($did, \f(CW$i\fP, \f(CW%features\fP)"
.IX Subsection "set_sentence_feature($did, $i, %features)"
.Vb 2
\&        Sets the given set of features for the given document $did and sentence $i.
\&        Returns undef if the sentence corresponding to $did, $i doesn\*(Aqt exist.
\&
\&        =cut
.Ve
.ie n .SS "score_sentences( combiner => $subref, normalize => 0, weights => \e%weights )"
.el .SS "score_sentences( combiner => \f(CW$subref\fP, normalize => 0, weights => \e%weights )"
.IX Subsection "score_sentences( combiner => $subref, normalize => 0, weights => %weights )"
.Vb 6
\&        Scores the sentences using the given combiner. A combiner subroutine will
\&        be passed a hash comtaining feature names mapped to values and should return
\&        a real number. By default, the sentence scores will be normalized unless
\&        normalize is set to 0. If the combiner does not return an appropriate value
\&        for each sentence, score_sentences returns undef and the sentence scores are
\&        left uncomputed.
\&
\&        Alternatively, if a hash reference is specified for the parameter weights, then
\&        the returned score will be a linear combination of the features specified
\&        in weights according to their given weights. This option will override the
\&        combiner parameter.
\&
\&        =head2 normalize_sentence_scores
\&
\&        Scales the scores of sentences such that the highest score is 1 and lowest is
\&        0. Returns undef if the scores are not defined.
\&
\&        =head2 get_text
\&
\&        Returns the text of each document concatenated together. A newline separates
\&        the text from each document.
\&
\&        =head2 sentence_scores_computed
\&
\&        Returns true if all of the sentence in this cluster have scores. False
\&        otherwise.
\&
\&        =cut
.Ve
.ie n .SS "compute_document_features( %features )"
.el .SS "compute_document_features( \f(CW%features\fP )"
.IX Subsection "compute_document_features( %features )"
.Vb 3
\&        Computes a set of features on all documents in the cluster. %features should
\&        be a hash mapping names to feature subroutine references. See
\&        compute_document_feature for more information.
\&
\&        =cut
.Ve
.ie n .SS "compute_document_feature( name => $name, feature => $subref )"
.el .SS "compute_document_feature( name => \f(CW$name\fP, feature => \f(CW$subref\fP )"
.IX Subsection "compute_document_feature( name => $name, feature => $subref )"
.Vb 3
\&        Computes the given feature for each document in the cluster. The feature
\&        parameter should be a reference to a subroutine. The subroutine will be
\&        called with the following parameters defined:
\&
\&        =over 2
\&
\&        =item * cluster \- a reference to the cluster object
\&
\&        =item * document \- a reference to the document object
\&
\&        =back
\&
\&        The parameter cluster is not passed when the same method is called on
\&        L<Clair::Document>. Thus calling compute_document_feature from Clair::Cluster
\&        gives an extra cluster context passed to the feature subroutine.
\&
\&        A feature subroutine should return a value. Any exceptions thrown by the
\&        feature subroutine will be caught and a warning will be shown. If a feature
\&        subroutine returns an undefined value, the feature will not be set and a
\&        warning will be shown. This method returns undef if either name or feature
\&        are not defined.
\&
\&        =cut
\&
\&        =head2 get_document_features
.Ve
.PP
\&\f(CW%features\fR = \f(CW$c\fR\->get_document_features($did)
.PP
.Vb 1
\&        Returns a hash mapping the features to values of document $did.
\&
\&        =cut
\&
\&        =head2 get_document_feature
.Ve
.PP
\&\f(CW$val\fR = \f(CW$c\fR\->get_document_feature($did, \f(CW$name\fR)
.PP
.Vb 3
\&        Returns the value of the given feature $name for document $did.
\&        Returns undef if $did isn\*(Aqt a valid document id, or $name isn\*(Aqt
\&        a valid feature.
\&
\&        =cut
\&
\&        =head2 remove_document_features
.Ve
.PP
\&\f(CW$c\fR\->\fIremove_document_features()\fR
.PP
.Vb 1
\&        Removes all of the features from all of the documents in the cluster.
\&
\&        =cut
\&
\&        =head2 set_document_feature
.Ve
.PP
\&\f(CW$c\fR\->set_document_feature($did, \f(CW%features\fR)
.PP
.Vb 2
\&        Sets the given set of features for the given document $did.
\&        Returns undef if the document corresponding to $did doesn\*(Aqt exist.
\&
\&        =cut
.Ve
.ie n .SS "get_summary(size => $size, preserve_order => 0, document_order => $ref)"
.el .SS "get_summary(size => \f(CW$size\fP, preserve_order => 0, document_order => \f(CW$ref\fP)"
.IX Subsection "get_summary(size => $size, preserve_order => 0, document_order => $ref)"
.Vb 4
\&        Returns a summary of this cluster based on the sentence scores. If the
\&        scores haven\*(Aqt been computed, it will return undef. A summary is an array
\&        of hash references. Each hash reference represents a sentence and contains
\&        the following key/value pairs:
\&
\&        =over 4
\&
\&        =item * did \- The document id of the document that this sentence came from
\&
\&        =item * index \- The index of this sentence in the document, starting at 0
\&
\&        =item * text \- The text of this sentence
\&
\&        =item * features \- A hash reference of this sentence\*(Aqs features
\&
\&        =item * score \- The score of this sentence.
\&
\&        =back
\&
\&        The size parameter sets the maximum length (number of sentences) of the
\&        summary.
\&
\&        The preserve_order parameter controls how the sentences are ordered. If
\&        preserve_order is set to 0, then the sentences will be returned in
\&        descending order by score. If two sentences have the same score and are from
\&        the same document, then they are returned such that the sentence with the
\&        lower index is first. If two sentences have the same score and are from
\&different documents, then the natural order (i.e., the perl cmp operator)
\&        on the documents\*(Aq ids will be used. If preserve_order is set to 1 or
\&        not defined, the order of the sentences in the summary is determined by
\&first sorting the sentences based on document (using cmp on the document ids)
\&        and then within the documents using the sentence index (preserving the
\&                        original order of sentences).
\&
\&        The order of the documents can be overridden by specifying an ordering on the
\&        document ids using the document_order parameter. If an array containing the
\&        list of document ids in some order is specified, then it will be used
\&        instead of perl\*(Aqs cmp operator to determine the order of the documents.
\&
\&        =head1 AUTHOR
\&
\&        Clair, C<< <clair at umich.edu> >>
\&
\&        =head1 BUGS
\&
\&        Please report any bugs or feature requests to
\&        C<bug\-clair\-cluster at rt.cpan.org>, or through the web interface at
\&        L<http://rt.cpan.org/NoAuth/ReportBug.html?Queue=clairlib\-dev>.
\&        I will be notified, and then you\*(Aqll automatically be notified of progress on
\&        your bug as I make changes.
\&
\&        =head1 SUPPORT
\&
\&        You can find documentation for this module with the perldoc command.
\&
\&        perldoc Clair::Document
\&
\&        You can also look for information at:
\&
\&        =over 4
\&
\&        =item * AnnoCPAN: Annotated CPAN documentation
\&
\&        L<http://annocpan.org/dist/clairlib\-dev>
\&
\&        =item * CPAN Ratings
\&
\&        L<http://cpanratings.perl.org/d/clairlib\-dev>
\&
\&        =item * RT: CPAN\*(Aqs request tracker
\&
\&        L<http://rt.cpan.org/NoAuth/Bugs.html?Dist=clairlib\-dev>
\&
\&        =item * Search CPAN
\&
\&        L<http://search.cpan.org/dist/clairlib\-dev>
\&
\&        =back
\&
\&        =head1 COPYRIGHT & LICENSE
\&
\&        Copyright 2006 Clair, all rights reserved.
\&
\&        This program is free software; you can redistribute it and/or modify it
\&        under the same terms as Perl itself.
\&
\&        =cut
\&
\&        1; # End of Clair::Cluster
.Ve
