\section{Mega Code Example}

Several code examples are provided with Clairlib, in the `test' directory
and also in the next section of the tutorial.  This section takes
a thorough look at one of these, `test\_mega.pl.'  This script
combines many pieces of functionality in Clairlib, so it serves as
a good example.

We now walk through this example section by section:
\\
\\
\begin{boxedverbatim}

# script: test_mega.pl
# functionality: Downloads documents using CorpusDownload, then makes IDFs,
# functionality: TFs, builds a cluster from them, a network based on a
# functionality: binary cosine, and tests the network for a couple of
# functionality: properties

use strict;
use warnings;
use FindBin;
use Clair::Utils::CorpusDownload;
use Clair::Utils::Idf;
use Clair::Utils::Tf;
use Clair::Document;
use Clair::Cluster;
use Clair::Network;

\end{boxedverbatim}
\\

We start by declaring the packages we will use.  We use FindBin
to make the example system independent, because we know the
relative location of the library to the script, rather than
the more typical situation of knowing the absolute path of the
library.  Typically, scripts
are more likely to change relative paths to the library than the
library is to move, so simply hard-coding the path here may be best
in most situations.

Next, we determine the ``base directory'' (where the script is located)
and remember the directory where we will put all produced files.  We
then create a CorpusDownload object, giving it a corpus name of ``testhtml''
and specifying the produced files directory as the root directory for the
corpus.  Note that we are specifying an absolute path, not a relative pass
for the rootdir parameter (otherwise, some CorpusDownload functions may not
work correctly).
\\
\\
\begin{boxedverbatim}

my $basedir = $FindBin::Bin;
my $gen_dir = "$basedir/produced/mega";

my $corpusref = Clair::Utils::CorpusDownload->new(corpusname => "testhtml",
                rootdir => $gen_dir);

\end{boxedverbatim}
\\

We use CorpusDownload::poach to start with a single URL and follow links
on that page, then links on those pages, etc. and return those URLs in
an array reference.  We iterate through those URLs and print them out
to the screen.  Finally, we pass those URLs to CorpusRef::buildCorpus
to download the URLs and create a corpus in TREC format.
\\
\\
\begin{boxedverbatim}

# Get the list of urls that we want to download
my $uref =                                                            \
$corpusref->poach("http://tangra.si.umich.edu/clair/testhtml/index.ht \
ml", error_file => "$gen_dir/errors.txt");

my @urls = @$uref;

foreach my $v (@urls) {
    print "URL: $v\n";
}

# Build the corpus using the list of urls
# This will index and convert to TREC format
$corpusref->buildCorpus(urlsref => $uref);


\end{boxedverbatim}
\\

Our next step is to build the IDF and TF files.  This computes the
IDF and TF values for every word, then stores them in files from which
those values can be easily retrieved.  We build the unstemmed IDF,
then the stemmed IDF first.  Next, we must build the DOCNO/URL
database before we build the TF files.  Again, we build the
unstemmed, and then the stemmed files (this order is not important
for either calculation).
\\
\\
\begin{boxedverbatim}

# -------------------------------------------------------------------
#  This is how to build the IDF.  First we build the unstemmed IDF,
#  then the stemmed one.
# -------------------------------------------------------------------
$corpusref->buildIdf(stemmed => 0);
$corpusref->buildIdf(stemmed => 1);

# -------------------------------------------------------------------
#  This is how to build the TF.  First we build the DOCNO/URL
#  database, which is necessary to build the TFs.  Then we build
#  unstemmed and stemmed TFs.
# -------------------------------------------------------------------
$corpusref->build_docno_dbm();
$corpusref->buildTf(stemmed => 0);
$corpusref->buildTf(stemmed => 1);

\end{boxedverbatim}
\\

Now that we have build these values, we want to be able to see what
the values are for specific words.  We create an Idf object, giving
it the same rootdir and corpusname as our CorpusDownload object.  We
choose whether we want the IDFs for the stemmed or unstemmed versions,
choosing unstemmed in this example.  We then get and print the IDF
values for several words: `have,' `and', and `zimbabwe.'  Note that
these words should be in lowercase.
\\
\\
\begin{boxedverbatim}

# -------------------------------------------------------------------
#  Here is how to use a IDF.  The constructor (new) opens the
#  unstemmed IDF.  Then we ask for IDFs for the words "have"
#  "and" and "zimbabwe."
# -------------------------------------------------------------------
my $idfref = Clair::Utils::Idf->new( rootdir => $gen_dir,
                       corpusname => "testhtml" ,
                       stemmed => 0 );

my $result = $idfref->getIdfForWord("have");
print "IDF(have) = $result\n";
$result = $idfref->getIdfForWord("and");
print "IDF(and) = $result\n";
$result = $idfref->getIdfForWord("zimbabwe");
print "IDF(zimbabwe) = $result\n";

\end{boxedverbatim}
\\

We now compute the TF values similarly.  We create a Tf object,
again using the same rootdir and corpusname as we did for
CorpusDownload, and again choosing whether we want the stemmed or
unstemmed information.  Now that we have our Tf object, we can
call getNumDocsWithWord to get the number of unique documents that
have a word, getFreq to get the number of times a word is in the
corpus, and getDocs to get all the URLs of all the documents that
have that word in them.  We do this with `washington', `and,' and
`zimbabwe.'
\\
\\
\begin{boxedverbatim}

# -------------------------------------------------------------------
#  Here is how to use a TF.  The constructor (new) opens the
#  unstemmed TF.  Then we ask for information about the
#  word "have":
#
#  1 first, we get the number of documents in the corpus with
#    the word "Washington"
#  2 then, we get the total number of occurrences of the word         \
"Washington"
#  3 then, we print a list of URLs of the documents that have the
#    word "Washington"
# -------------------------------------------------------------------
my $tfref = Clair::Utils::Tf->new( rootdir => $gen_dir,
                     corpusname => "testhtml" ,
                     stemmed => 0 );

$result = $tfref->getNumDocsWithWord("washington");
my $freq   = $tfref->getFreq("washington");
@urls = $tfref->getDocs("washington");
print "TF(washington) = $freq total in $result docs\n";
print "Documents with \"washington\"\n";
foreach my $url (@urls)  {  print "  $url\n";  }
print "\n";

# -------------------------------------------------------------------
#  Then we do 1-2 with the word "and"
# -------------------------------------------------------------------
$result = $tfref->getNumDocsWithWord("and");
$freq   = $tfref->getFreq("and");
@urls = $tfref->getDocs("and");
print "TF(and) = $freq total in $result docs\n";

# -------------------------------------------------------------------
#  Then we do 1-3 with the word "zimbabwe"
# -------------------------------------------------------------------
$result = $tfref->getNumDocsWithWord("zimbabwe");
$freq   = $tfref->getFreq("zimbabwe");
@urls = $tfref->getDocs("zimbabwe");
print "TF(zimbabwe) = $freq total in $result docs\n";
print "Documents with \"zimbabwe\"\n";
foreach my $url (@urls)  {  print "  $url\n";  }
print "\n";

\end{boxedverbatim}
\\

We now change direction, using the fact that CorpusDownload has
downloaded all of the html files to a specific directory.  The
directory location depends upon the root directory, the corpusname
and the url of each downloaded file.  In this case, all the
downloaded files are from the same host and same path in the URL,
so they are all in the same folder.

We create a new Clair::Cluster and use load\_documents to get all
the files from that directory.  We give a type of `html' so that
every Clair::Document that is created has type `html.'  Once we
have loaded the documents, we display a message saying how many
we have, then strip and stem all the documents.
\\
\\
\begin{boxedverbatim}

# Create a cluster with the documents
my $c = new Clair::Cluster;

$c->load_documents("$gen_dir/download/testhtml/tangra.si.umich.edu/cl \
air/testhtml/*", type => 'html');

print "Loaded ", $c->count_elements, " documents.\n";

$c->strip_all_documents;
$c->stem_all_documents;

print "I'm done stripping and stemming\n";


\end{boxedverbatim}
\\

In order to shorten the computation for the rest of the example,
we only want to look at 40 of the documents.  To do this, we
simply use a foreach loop that inserts the first 40 documents
into a new cluster.  Which 40 documents are inserted will vary
from system to system (and possibly from run to run) since they
are not specified or explicitly ordered in any way.
\\
\\
\begin{boxedverbatim}

my $count = 0;
my $c2 = new Clair::Cluster;
foreach my $doc (values %{ $c->documents} ) {
    $count++;

    if ($count <= 40) {
        $c2->insert($doc->get_id, $doc);
    }
}

\end{boxedverbatim}
\\

We now compute the cosine matrix for the new cluster.  This will
return a hash.  By indexing into the hash using a pair of documents,
we can get the cosine similarity of those two documents.  We next
compute the binary cosine using a threshold of 0.15.  We could
specify the cosine matrix, but not specifying it will result in the use of the
cosine matrix from the last compute\_cosine\_matrix.  This returns
a hash with the same format as that returned by compute\_cosine\_matrix.

Next, we create a network based on the binary cosine.  Every document
with at least one edge (explained next) will become a vertex
in the network, and every pair of documents
with a non-zero cosine matrix will have an edge between their
corresponding vertices.

Using this network, we compute a few statistics, getting the number
of documents in the network (remember, this will probably be less
than the 40 we started with because it is the number of documents
with at least one edge).  We also print out the average and
maximum diameter of the network we created.
\\
\\
\begin{boxedverbatim}

my %cm = $c2->compute_cosine_matrix();
my %bin_cos = $c2->compute_binary_cosine(0.15);
my $network = $c2->create_network(cosine_matrix => \%bin_cos);

print "Number of documents in network: ", $network->num_documents,    \
"\n";

print "Average diameter: ", $network->diameter(avg => 1), "\n";
print "Maximum diameter: ", $network->diameter(), "\n";

\end{boxedverbatim}











\end{document}
